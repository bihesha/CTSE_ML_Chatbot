{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "af37bf8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "import os\n",
    "import time\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import DirectoryLoader, PyPDFLoader, UnstructuredPowerPointLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "# Set your Google API Key\n",
    "def set_api_key(api_key=None):\n",
    "    if api_key is None:\n",
    "        api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
    "    if not api_key:\n",
    "        print(\"API key not found!\")\n",
    "        return\n",
    "    os.environ[\"GOOGLE_API_KEY\"] = api_key\n",
    "    print(\"API Key Successfully Completed\")\n",
    "\n",
    "# Start the chatbot with ability to add new files to existing DB\n",
    "def initialize_chatbot(force_reindex=False):\n",
    "    print(\"Starting CTSE Chatbot...\")\n",
    "\n",
    "    # Define the prompt template\n",
    "    template = \"\"\"\n",
    "    As an expert assistant for the CTSE course, provide concise, focused answers to course material, \n",
    "    avoiding making up answers if you don't know the answer.\n",
    "\n",
    "    Context:\n",
    "    {context}\n",
    "\n",
    "    Question: {question}\n",
    "\n",
    "    Answer:\n",
    "    \"\"\"\n",
    "\n",
    "    prompt = PromptTemplate(\n",
    "        template=template,\n",
    "        input_variables=[\"context\", \"question\"]\n",
    "    )\n",
    "\n",
    "    # Initialize the LLM\n",
    "    llm = ChatGoogleGenerativeAI(\n",
    "        model=\"gemini-1.5-flash\",\n",
    "        temperature=0.2,\n",
    "    )\n",
    "\n",
    "    # Setup paths and embedding model\n",
    "    db_path = \"./Chroma_db\"\n",
    "    lecture_notes_path = \"data_sets\" \n",
    "\n",
    "    # Initialize embedding model\n",
    "    print(\"Starting embedding model...\")\n",
    "    embedding_model = HuggingFaceEmbeddings(\n",
    "        model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "        model_kwargs={\"device\": \"cpu\"}\n",
    "    )\n",
    "\n",
    "    # Load all PDF and PPTX documents from the directory\n",
    "    print(f\"Scanning {lecture_notes_path} folder for lecture materials...\")\n",
    "\n",
    "    if not os.path.exists(lecture_notes_path):\n",
    "        print(f\"Error: Lecture notes directory not found: {lecture_notes_path}\")\n",
    "        print(\"Please create this directory and add your lecture notes (PDFs or PPTXs).\")\n",
    "        return None\n",
    "\n",
    "    # Load PDF files\n",
    "    pdf_loader = DirectoryLoader(\n",
    "        lecture_notes_path,\n",
    "        glob=\"**/*.pdf\",\n",
    "        loader_cls=PyPDFLoader\n",
    "    )\n",
    "    pdf_docs = pdf_loader.load()\n",
    "\n",
    "    # Load PPTX files\n",
    "    pptx_loader = DirectoryLoader(\n",
    "        lecture_notes_path,\n",
    "        glob=\"**/*.pptx\",\n",
    "        loader_cls=UnstructuredPowerPointLoader\n",
    "    )\n",
    "    pptx_docs = pptx_loader.load()\n",
    "\n",
    "    # Combine documents\n",
    "    all_documents = pdf_docs + pptx_docs\n",
    "\n",
    "    if not all_documents:\n",
    "        print(\"Error: No documents (PDF/PPTX) found in the lecture notes directory.\")\n",
    "        return None\n",
    "\n",
    "    print(f\"Total documents found: {len(all_documents)}\")\n",
    "\n",
    "    # Process documents based on DB existence\n",
    "    if os.path.exists(db_path) and len(os.listdir(db_path)) > 0 and not force_reindex:\n",
    "        print(\"Existing vector database found.\")\n",
    "\n",
    "        print(\"Loading existing Chroma DB...\")\n",
    "        vectorstore = Chroma(\n",
    "            persist_directory=db_path,\n",
    "            embedding_function=embedding_model\n",
    "        )\n",
    "\n",
    "        # Get list of documents already in the DB\n",
    "        print(\"Checking for new documents to embed...\")\n",
    "        existing_docs = set(vectorstore.get()[\"metadatas\"])\n",
    "        existing_sources = set()\n",
    "\n",
    "        for doc in existing_docs:\n",
    "            if doc and \"source\" in doc:\n",
    "                existing_sources.add(doc[\"source\"])\n",
    "\n",
    "        print(f\"Found {len(existing_sources)} documents already embedded.\")\n",
    "\n",
    "        # Filter new documents\n",
    "        new_documents = []\n",
    "        for doc in all_documents:\n",
    "            if doc.metadata.get(\"source\") not in existing_sources:\n",
    "                new_documents.append(doc)\n",
    "\n",
    "        print(f\"Found {len(new_documents)} new documents to embed.\")\n",
    "\n",
    "        if new_documents:\n",
    "            print(\"Processing new documents...\")\n",
    "            text_splitter = RecursiveCharacterTextSplitter(\n",
    "                chunk_size=1000,\n",
    "                chunk_overlap=200\n",
    "            )\n",
    "            new_splits = text_splitter.split_documents(new_documents)\n",
    "            print(f\"Created {len(new_splits)} chunks from new documents.\")\n",
    "\n",
    "            print(\"Adding new documents to Chroma DB...\")\n",
    "            vectorstore.add_documents(new_splits)\n",
    "            print(\"Successfully added new documents to existing database...!\")\n",
    "        else:\n",
    "            print(\"No new documents to add.\")\n",
    "    else:\n",
    "        if force_reindex:\n",
    "            print(\"Full reindexing initiated by user request.\")\n",
    "        else:\n",
    "            print(\"No existing Chroma DB found or it's empty.\")\n",
    "\n",
    "        print(\"Creating new vector database from all documents...\")\n",
    "\n",
    "        print(\"Splitting documents into text chunks for vector embedding...\")\n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=1000,\n",
    "            chunk_overlap=200\n",
    "        )\n",
    "        splits = text_splitter.split_documents(all_documents)\n",
    "        print(f\"Chunking complete: {len(splits)} sections generated from {len(all_documents)} documents.\")\n",
    "\n",
    "        print(\"Embedding documents (this may take some time)...\")\n",
    "        start_time = time.time()\n",
    "        vectorstore = Chroma.from_documents(\n",
    "            documents=splits,\n",
    "            embedding=embedding_model,\n",
    "            persist_directory=db_path\n",
    "        )\n",
    "        end_time = time.time()\n",
    "        print(f\"Embedding completed in {end_time - start_time:.2f}s.\")\n",
    "\n",
    "    # Create QA chain\n",
    "    print(\"Building intelligent QA system from embedded knowledge...\")\n",
    "    qa_chain = RetrievalQA.from_chain_type(\n",
    "        llm=llm,\n",
    "        chain_type=\"stuff\",\n",
    "        retriever=vectorstore.as_retriever(search_kwargs={\"k\": 4}),\n",
    "        chain_type_kwargs={\"prompt\": prompt},\n",
    "        return_source_documents=True\n",
    "    )\n",
    "\n",
    "    print(\"Chatbot Started successful and ready to answer queries....!\")\n",
    "    return qa_chain\n",
    "\n",
    "# Function to ask a question and get an answer\n",
    "def ask_question(qa_chain, question):\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(f\" --- CTSE Chatbot ---\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"üìå Question: {question}\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "    try:\n",
    "        print(\"üîé Scanning documents for the best match...\")\n",
    "        start_time = time.time()\n",
    "        result = qa_chain({\"query\": question})\n",
    "        end_time = time.time()\n",
    "\n",
    "        answer = result[\"result\"]\n",
    "        source_docs = result[\"source_documents\"]\n",
    "\n",
    "        print(f\"\\n‚úÖ Answer: {answer}\")\n",
    "        print(f\"\\n --- Response generated in {end_time - start_time:.2f} seconds ---\")\n",
    "\n",
    "        if source_docs:\n",
    "            print(\"\\nüìö Sources:\")\n",
    "            unique_sources = list({os.path.basename(doc.metadata.get('source', 'Unknown')) for doc in source_docs})\n",
    "            for idx, source in enumerate(unique_sources[:3], 1):\n",
    "                print(f\"  {idx}. {source}\")\n",
    "\n",
    "        print(\"=\" * 60)\n",
    "        return result\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error while processing your question: {str(e)}\")\n",
    "        print(\"=\" * 60)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2237bf62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API Key Successfully Completed\n",
      "Starting CTSE Chatbot...\n",
      "Starting embedding model...\n",
      "Scanning data_sets folder for lecture materials...\n",
      "Total documents found: 630\n",
      "Full reindexing initiated by user request.\n",
      "Creating new vector database from all documents...\n",
      "Splitting documents into text chunks for vector embedding...\n",
      "Chunking complete: 1102 sections generated from 630 documents.\n",
      "Embedding documents (this may take some time)...\n",
      "Embedding completed in 55.85s.\n",
      "Building intelligent QA system from embedded knowledge...\n",
      "Chatbot Started successful and ready to answer queries....!\n"
     ]
    }
   ],
   "source": [
    "api_key = \"AIzaSyCnoTSZVnaZJoHVMVGeMGWNXDMAsQTCndU\"  # Replace with your actual API key\n",
    "set_api_key(api_key)\n",
    "\n",
    "qa_chain = initialize_chatbot(force_reindex=True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "72696ece",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      " --- CTSE Chatbot ---\n",
      "============================================================\n",
      "üìå Question: can you tell me what is Docker?\n",
      "------------------------------------------------------------\n",
      "üîé Scanning documents for the best match...\n",
      "\n",
      "‚úÖ Answer: Docker is a container engine (runtime + tool for managing containers and images) that packages and runs applications in isolated environments called containers.  It provides a CLI tool, platform, and company offerings.\n",
      "\n",
      " --- Response generated in 3.03 seconds ---\n",
      "\n",
      "üìö Sources:\n",
      "  1. Containers 101.pptx\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "question = \"can you tell me what is Docker?\"\n",
    "\n",
    "result = ask_question(qa_chain, question)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
